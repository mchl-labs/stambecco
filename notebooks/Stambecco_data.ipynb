{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgJXS62reiED"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from tenacity import (\n",
        "    retry, \n",
        "    stop_after_attempt, \n",
        "    wait_fixed, \n",
        "    retry_if_exception_type, \n",
        "    wait_random_exponential,\n",
        "    )\n",
        "\n",
        "# Use always the latest stable version of the Alpaca Dataset cleaned\n",
        "url = 'https://raw.githubusercontent.com/gururise/AlpacaDataCleaned/main/alpaca_data_cleaned.json'\n",
        "# OR of the GPT-4-LLM Alpaca Dataset\n",
        "# url = 'https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json'\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_fixed(0.1),\n",
        "      retry=retry_if_exception_type(requests.HTTPError))\n",
        "def get(url):\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()  # raise an error on a bad status\n",
        "        return r\n",
        "    except requests.HTTPError:\n",
        "        print(r.status_code, r.reason)\n",
        "        raise\n",
        "\n",
        "response = get(url)\n",
        "\n",
        "print(f\"Request returned {response.status_code} : '{response.reason}'\")\n",
        "\n",
        "if(response.status_code == 200):\n",
        "  payload = response.json()\n",
        "else:\n",
        "  print(\"Error\")\n",
        "\n",
        "payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUX-weKGfcDi"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap1jc6kofcm2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "OPENAI_API_KEY = 'YOUR_API_KEY_GOES_HERE'\n",
        "\n",
        "# ISO 639-1 language code.\n",
        "# See https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n",
        "\n",
        "lang = \"it\"\n",
        "# The language name for OpenAI\n",
        "LANGUAGE = \"Italian\"\n",
        "\n",
        "# Set to True if you want to split the dataset in batches. Reduces peak memory footprint. DEFAULT: False\n",
        "SPLIT_DATA = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FwABUQIfjVk"
      },
      "outputs": [],
      "source": [
        "def matches_regex(regex, text):\n",
        "    return bool(re.compile(regex).search(text))\n",
        "\n",
        "\n",
        "def contains_code(text):\n",
        "    # filter based on keywords that indicate code\n",
        "    code_blacklist = ['&&', '||', '<html>', ';\\n', 'SELECT', \"{\", \"[\"]\n",
        "    \n",
        "    return (\n",
        "            any(code_keyword in text for code_keyword in code_blacklist) |\n",
        "            matches_regex(r'\\w+\\(\\w*\\) \\{', text) | # e.g. myFunc() {\n",
        "            matches_regex(r'def \\w+\\(', text) | # e.g. def parse_list(\n",
        "            matches_regex(r'\\[A-z]+\\.[A-z]+', text) | # e.g. this.language\n",
        "            matches_regex(r': [\\w\\.#]{1,12};', text) | # e.g. font-size: 1.3em;\n",
        "            matches_regex(r'<\\/\\w+>', text) # e.g. </html>\n",
        "           )\n",
        "\n",
        "\n",
        "def contains_words(text):\n",
        "    return matches_regex(r'[A-z]{3,}', text) # words with at least three characters\n",
        "\n",
        "\n",
        "def is_translatable(text):\n",
        "    if text == \"\":\n",
        "        return False\n",
        "    return (contains_code(text) is False) & contains_words(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lyzZp5uhCAu"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(3))\n",
        "def translate_OpenAI(value):\n",
        "    \"\"\"\n",
        "    Translates text into the target language using Open AI gpt-3.5-turbo model.\n",
        "    \"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Translate the following text from English to {LANGUAGE}: '{value}'\"},\n",
        "            ],\n",
        "        max_tokens=1024,\n",
        "        temperature=0,\n",
        "        )\n",
        "    return response.choices[0][\"message\"][\"content\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNXFqVjwuwbR"
      },
      "outputs": [],
      "source": [
        "def merge_json_files(folder_path):\n",
        "    json_files = [pos_json for pos_json in os.listdir(folder_path) if pos_json.endswith('.json')]\n",
        "    json_data = []\n",
        "    for file in json_files:\n",
        "        with open(os.path.join(folder_path, file)) as f:\n",
        "            json_data.extend(json.load(f))\n",
        "    return json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOn9aYTKf12Z"
      },
      "outputs": [],
      "source": [
        "def translate_text(value):\n",
        "  if(is_translatable(value)): \n",
        "    return translate_OpenAI(value)\n",
        "  return value\n",
        "\n",
        "def translate_item(item):\n",
        "    translated_item = {}\n",
        "    for key, value in item.items():\n",
        "        if value:\n",
        "            translated_value = translate_text(value)\n",
        "            translated_item[key] = translated_value\n",
        "        else:\n",
        "            translated_item[key] = ''\n",
        "    return translated_item\n",
        "\n",
        "# Maximum number of parallel requests\n",
        "if(SPLIT_DATA is False):\n",
        "    MAX_PARALLEL_REQUESTS = 20\n",
        "else:\n",
        "    MAX_PARALLEL_REQUESTS = 50\n",
        "\n",
        "data = payload\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "start = 0\n",
        "end = len(data)\n",
        "print(len(data))\n",
        "\n",
        "if(SPLIT_DATA is False):\n",
        "    translated_data = []\n",
        "    data = data[start:end]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_PARALLEL_REQUESTS) as executor:\n",
        "        futures = {executor.submit(translate_item, item): item for item in data}\n",
        "        \n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating\"):\n",
        "            translated_data.append(future.result())\n",
        "\n",
        "    # Save the translated data to a new JSON file named 'stambecco_data_{lang}.json'\n",
        "    with open(f'stambecco_data_{lang}.json', 'w') as f:\n",
        "        json.dump(translated_data, f, ensure_ascii=False, indent=4)\n",
        "else:\n",
        "    # Translate the data in chunks\n",
        "    for i in range(start, end, CHUNK_SIZE):\n",
        "        start = i\n",
        "        end = i + CHUNK_SIZE\n",
        "\n",
        "        translated_data = []\n",
        "        data_new = data[start:end]\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=MAX_PARALLEL_REQUESTS) as executor:\n",
        "            futures = {executor.submit(translate_item, item): item for item in data_new}\n",
        "\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating\"):\n",
        "                translated_data.append(future.result())\n",
        "\n",
        "\n",
        "        # Save the translated data to a new JSON file named 'translated_data_from_{start}_to_{end}.json'\n",
        "        with open(f'./data/translated_data_up_to_{start}_to_{end}.json', 'w') as f:\n",
        "            json.dump(translated_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"Translation complete. The translated data is saved in 'translated_data_from_{start}_to_{end}.json'\")\n",
        "\n",
        "    folder_path = './data'\n",
        "    merged_data = merge_json_files(folder_path)\n",
        "    with open(f'stambecco_data_{lang}.json', 'w') as f:\n",
        "        json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Translation complete. The translated data is saved in 'stambecco_data_{lang}.json'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
