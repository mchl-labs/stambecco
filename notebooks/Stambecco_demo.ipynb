{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_gvN04-1hzs"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece accelerate\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34KnEgLZ1r3x"
      },
      "outputs": [],
      "source": [
        "# Please specify a BASE_MODEL, e.g. BASE_MODEL = 'decapoda-research/llama-7b-hf'\n",
        "BASE_MODEL = \"\"\n",
        "\n",
        "# Please specify a LORA_WEIGHTS, e.g. LORA_WEIGHTS = 'mchl-labs/stambecco-7b-plus'\n",
        "LORA_WEIGHTS = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-yQ_uN41mIi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import gradio as gr\n",
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "\n",
        "assert torch.cuda.is_available(), \"Change the runtime type to GPU, disable this check if you are on 'mps'\"\n",
        "device = \"cuda\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:\n",
        "    pass\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "if device == \"mps\":\n",
        "    model = LLaMAForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        LORA_WEIGHTS,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "else:\n",
        "    model = LLaMAForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        load_in_8bit=True,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DOBIxYA2In9"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Di seguito Ã¨ riportata un'istruzione che descrive un task, insieme ad un input che fornisce un contesto piÃ¹ ampio. Scrivi una risposta che completi adeguatamente la richiesta.\n",
        "### Istruzione:\n",
        "{instruction}\n",
        "### Input:\n",
        "{input}\n",
        "### Risposta:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Di seguito Ã¨ riportata un'istruzione che descrive un task. Scrivi una risposta che completi adeguatamente la richiesta.\n",
        "### Istruzione:\n",
        "{instruction}\n",
        "### Risposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import traceback\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "\n",
        "\n",
        "class Stream(transformers.StoppingCriteria):\n",
        "    def __init__(self, callback_func=None):\n",
        "        self.callback_func = callback_func\n",
        "\n",
        "    def __call__(self, input_ids, scores) -> bool:\n",
        "        if self.callback_func is not None:\n",
        "            self.callback_func(input_ids[0])\n",
        "        return False\n",
        "\n",
        "\n",
        "class Iteratorize:\n",
        "\n",
        "    \"\"\"\n",
        "    Transforms a function that takes a callback\n",
        "    into a lazy iterator (generator).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, func, kwargs={}, callback=None):\n",
        "        self.mfunc = func\n",
        "        self.c_callback = callback\n",
        "        self.q = Queue()\n",
        "        self.sentinel = object()\n",
        "        self.kwargs = kwargs\n",
        "        self.stop_now = False\n",
        "\n",
        "        def _callback(val):\n",
        "            if self.stop_now:\n",
        "                raise ValueError\n",
        "            self.q.put(val)\n",
        "\n",
        "        def gentask():\n",
        "            try:\n",
        "                ret = self.mfunc(callback=_callback, **self.kwargs)\n",
        "            except ValueError:\n",
        "                pass\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "                pass\n",
        "\n",
        "            self.q.put(self.sentinel)\n",
        "            if self.c_callback:\n",
        "                self.c_callback(ret)\n",
        "\n",
        "        self.thread = Thread(target=gentask)\n",
        "        self.thread.start()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        obj = self.q.get(True, None)\n",
        "        if obj is self.sentinel:\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.stop_now = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.config.pad_token_id = tokenizer.pad_token_id = 0\n",
        "model.config.bos_token_id = 1\n",
        "model.config.eos_token_id = 2\n",
        "\n",
        "def evaluate(\n",
        "        instruction,\n",
        "        input=None,\n",
        "        temperature=0.1,\n",
        "        top_p=0.75,\n",
        "        top_k=40,\n",
        "        num_beams=4,\n",
        "        max_new_tokens=128,\n",
        "        stream_output=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        prompt = generate_prompt(instruction, input)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].to(device)\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            num_beams=num_beams,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        generate_params = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"generation_config\": generation_config,\n",
        "            \"return_dict_in_generate\": True,\n",
        "            \"output_scores\": True,\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "        }\n",
        "\n",
        "        if stream_output:\n",
        "            # Stream the reply 1 token at a time.\n",
        "            # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
        "            # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
        "\n",
        "            def generate_with_callback(callback=None, **kwargs):\n",
        "                kwargs.setdefault(\n",
        "                    \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
        "                )\n",
        "                kwargs[\"stopping_criteria\"].append(\n",
        "                    Stream(callback_func=callback)\n",
        "                )\n",
        "                with torch.no_grad():\n",
        "                    model.generate(**kwargs)\n",
        "\n",
        "            def generate_with_streaming(**kwargs):\n",
        "                return Iteratorize(\n",
        "                    generate_with_callback, kwargs, callback=None\n",
        "                )\n",
        "\n",
        "            with generate_with_streaming(**generate_params) as generator:\n",
        "                for output in generator:\n",
        "                    decoded_output = tokenizer.decode(output)\n",
        "\n",
        "                    if output[-1] in [tokenizer.eos_token_id]:\n",
        "                        break\n",
        "\n",
        "                    yield decoded_output.split(\"### Risposta:\")[1].strip()\n",
        "            return  # early return for stream_output\n",
        "\n",
        "        # Without streaming\n",
        "        with torch.no_grad():\n",
        "            generation_output = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "            )\n",
        "        s = generation_output.sequences[0]\n",
        "        output = tokenizer.decode(s)\n",
        "        yield output.split(\"### Risposta:\")[1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV25YBC_2O42"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "g = gr.Interface(\n",
        "    fn=evaluate,\n",
        "    inputs=[\n",
        "        gr.components.Textbox(\n",
        "            lines=2, label=\"Instruction\", placeholder=\"Parlami della fondazione di Roma\"\n",
        "        ),\n",
        "        gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"Inserisci qui il tuo input\"),\n",
        "        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n",
        "        gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n",
        "        gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n",
        "        gr.components.Slider(minimum=1, maximum=4, step=1, value=4, label=\"Beams\"),\n",
        "        gr.components.Slider(\n",
        "            minimum=1, maximum=512, step=1, value=128, label=\"Max tokens\"\n",
        "        ),\n",
        "        gr.components.Checkbox(value=True, label=\"Stream output\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.inputs.Textbox(\n",
        "            lines=7,\n",
        "            label=\"Output\",\n",
        "        )\n",
        "    ],\n",
        "    title=\"ðŸ¦Œ Stambecco ðŸ‡®ðŸ‡¹\",\n",
        "    description=\"Stambecco is a Italian Instruction-following model based on the LLaMA model. It is trained on an Italian version of the [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) dataset, a dataset of GPT-4 generated instruction-following data. For more information, please visit [the project's website](https://github.com/mchl-labs/stambecco).\",\n",
        "    )\n",
        "g.queue().launch(server_name=\"0.0.0.0\", share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
