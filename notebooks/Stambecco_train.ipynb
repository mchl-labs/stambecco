{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ax11v71pnCL"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece accelerate\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mDZNgsRptSm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, LLaMAForCausalLM, LLaMATokenizer\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-5aauj5pw94"
      },
      "outputs": [],
      "source": [
        "MICRO_BATCH_SIZE = 4\n",
        "BATCH_SIZE = 128\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 2  # we don't need 3 tbh\n",
        "LEARNING_RATE = 3e-4  # the Karpathy constant\n",
        "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Please specify a BASE_MODEL, e.g. BASE_MODEL = 'decapoda-research/llama-7b-hf'\n",
        "BASE_MODEL = \"\"\n",
        "# Insert the path to your data\n",
        "DATA_PATH = \"mchl-labs/stambecco_data_plus_it\"\n",
        "# Insert the name of your model\n",
        "NAME = \"stambecco-7b-plus\"\n",
        "# Insert your HUGGINGFACE HUB Token\n",
        "os.environ[\"HUB_TOKEN\"] = \"YOUR_HUB_TOKEN_HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hub_token = os.environ[\"HUB_TOKEN\"]\n",
        "print(f\"Hub token: {hub_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGFXGMh-p4K1"
      },
      "outputs": [],
      "source": [
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(\n",
        "    BASE_MODEL, add_eos_token=True\n",
        ")\n",
        "\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "tokenizer.pad_token_id = 0  # We want this to be different from the eos token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw86GLzcrs38"
      },
      "outputs": [],
      "source": [
        "if DATA_PATH.endswith(\".json\") or DATA_PATH.endswith(\".jsonl\"):\n",
        "        data = load_dataset(\"json\", data_files=DATA_PATH)\n",
        "else:\n",
        "        data = load_dataset(DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upbBEQadp9Xy"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    if data_point[\"input\"]:\n",
        "        return f\"\"\"Di seguito è riportata un'istruzione che descrive un task, insieme ad un input che fornisce un contesto più ampio. Scrivi una risposta che completi adeguatamente la richiesta.\n",
        "### Istruzione:\n",
        "{data_point[\"instruction\"]}\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "### Risposta:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Di seguito è riportata un'istruzione che descrive un task. Scrivi una risposta che completi adeguatamente la richiesta.\n",
        "### Istruzione:\n",
        "{data_point[\"instruction\"]}\n",
        "### Risposta:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(prompt):\n",
        "    # there's probably a way to do this with the tokenizer settings\n",
        "    # but again, gotta move fast\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN + 1,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": result[\"input_ids\"][:-1],\n",
        "        \"attention_mask\": result[\"attention_mask\"][:-1],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O92BGbI0qJjc"
      },
      "outputs": [],
      "source": [
        "data = data.shuffle().map(lambda x: tokenize(generate_prompt(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOAYN2smqTLS"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        logging_steps=20,\n",
        "        optim=\"adamw_torch\",\n",
        "        output_dir=NAME,\n",
        "        save_total_limit=3,\n",
        "        save_strategy=\"epoch\",\n",
        "        # push_to_hub=True,\n",
        "        # hub_token=hub_token,\n",
        "        # hub_private_repo=True,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmPE5CwZ0YOk"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(f\"models/{NAME}\")\n",
        "# trainer.push_to_hub(language=\"it\", license=\"cc-by-nc-nd-4.0\", finetuned_from=\"llama-7b\", dataset=DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.push_to_hub(repo_id=NAME, use_auth_token=hub_token)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
